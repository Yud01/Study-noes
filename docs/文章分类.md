# 贝叶斯推断（Bayesian inference）

**朴素贝叶斯**是一种**概率分类器**，当我们对某个文档 \(d\) 进行分类，在所有类别集合 C 中，分类器返回的类别 c hat 是在**后验概率**下最可能的类别。  
这里，c hat 上的“帽子”表示我们估计的“正确”类别，符号 **argmax** 表示使得目标函数取最大值的参数（本式中即类别 \(c\)）：

$$
\hat{c}=\arg\max_{c\in\mathcal{C}} P(c\mid d) \tag{4.1}
$$
这种**贝叶斯推断**的思想最早由 **Bayes (1763)** 提出，之后被 **Mosteller & Wallace (1964)** 成功应用于文本分类的研究。

---

贝叶斯分类的核心思想是利用**贝叶斯定理**把式 (4.1) 中的条件概率P (c | d)转换成更便于计算的形式。贝叶斯定理的一般形式为，将任意条件概率P (x | y) 分解为另外三个概率：

$$
P(x\mid y)=\frac{P(y\mid x)\,P(x)}{P(y)} \tag{4.2}
$$

---

在本节我们介绍**多项式朴素贝叶斯分类器**（*Multinomial Naive Bayes Classifier*）。它是贝叶斯学习中一种基本、经典的“朴素”分类器，之所以称为“朴素”，是因为它对**特征之间的依赖关系做了简化假设**（即**条件独立**假设）。

---

将式 (4.2) 代入式 (4.1)，得到：

$$
\hat{c}
=\arg\max_{c\in\mathcal{C}} P(c\mid d)
=\arg\max_{c\in\mathcal{C}} \frac{P(d\mid c)\,P(c)}{P(d)} \tag{4.3}
$$

由于我们进行的是**取最大值**运算，且对于每一个可能的类别 \(c\) 来说，分母 \(P(d)\) 与 \(c\) **无关**（对同一个文档 \(d\) 是常数），因此可以在比较时**忽略**它，于是得到常用的等价形式：

$$
\hat{c}
=\arg\max_{c\in\mathcal{C}} P(d\mid c)\,P(c) \tag{4.4}
$$

---

我们把朴素贝叶斯称作一种**生成式（generative）模型**。从式 (4.4) 可以读出这样一种隐含假设：
先从 P(c)中抽取一个类别 \(c\)，再根据 P(d | c) 为该类别**生成**文档（或至少生成该文档的词计数）。
从这个过程的角度出发，我们甚至可以想象生成“人工文档”。

回到**分类**任务：给定某个文档 \(d\)，我们选择使**先验概率** \(P(c)\) 与**似然** P(d | c)的**乘积**最大的类别 c hat：

$$
\hat{c}=\arg\max_{c\in\mathcal{C}} P(d\mid c)\, \widehat{P}(c) \tag{4.5}
$$

> 术语提示：  
> - *prior probability*（先验）\(P(c)\)  
> - *likelihood*（似然）P(d|c)

为不失一般性，我们也可以把文档 \(d\) 表示为一组特征 f1 f2 f3 ...。此时对应的判别式写为：

$$
\hat{c}=\arg\max_{c\in\mathcal{C}} P(f_1,f_2,\ldots,f_n\mid c)\, \widehat{P}(c) \tag{4.6}
$$

然而，直接计算式 (4.6) 往往仍然非常困难：若不作任何简化，要估计“**每一种可能的特征组合**”（例如所有可能的词及其位置组合）的概率将需要巨量的参数，并导致无法接受的训练样本规模。因此，朴素贝叶斯采用两个**简化假设**：

1. **词袋（bag‑of‑words）假设**：词在文档中的**位置不重要**，比如单词 *love* 出现在第 1 个、第 20 个或最后一个位置，对分类的影响视为相同。于是我们让特征 (f_1,...) **仅编码词的身份**，而不编码位置信息。  
2. **朴素贝叶斯假设（条件独立）**：给定类别 c 时，各特征 P(f_i|c)**相互独立**，因此可以“朴素地”把联合概率拆成乘积：

$$
P(f_1,f_2,\ldots,f_n\mid c)\;=\;P(f_1\mid c)\cdot P(f_2\mid c)\cdot \ldots \cdot P(f_n\mid c) \tag{4.7}
$$

基于以上两点，朴素贝叶斯分类器选择的最终类别公式为：

$$
c_{\mathrm{NB}} \;=\; \arg\max_{c\in\mathcal{C}} P(c)\,\prod_{f\in F} P(f\mid c) \tag{4.8}
$$

其中 \(F\) 为文档的特征集合（在词袋模型中，可理解为文档中出现的词/词频等特征）。

4.2 朴素贝叶斯（对数空间与线性分类器；参数学习）

通过在**对数空间**考虑特征，式 (4.10) 可以把预测类别写成输入特征的**线性函数**。  
凡是用**输入的线性组合**来做分类决策的模型——比如朴素贝叶斯和逻辑回归——都称为**线性分类器**。

> 注：除非特别说明，本文中的 `log` 指**自然对数** \(\ln\)。

---

## 训练朴素贝叶斯分类器

我们如何学习 P(c)与 P(f_i | c)呢？先看**极大似然估计（MLE）**：直接用训练数据中的频率。

### 先验 \(P(c)\)

令 N_c 为训练集中属于类别 \(c\) 的文档数，(N_doc) 
为训练集的**总**文档数，则
$$
\widehat{P}(c)=\frac{N_c}{N_{\text{doc}}}\tag{4.11}
$$

### 似然 P(f_i | c)（词袋视角）

把**特征**视为“文档的词袋中某个词是否出现（或出现次数）”。于是我们要估计 P(w_i | c)。做法：把**所有**类别为 c 的训练文档**拼接**成一个“大文档”，用其中词w_i的**频率**做极大似然估计：
$$
\widehat{P}(w_i\mid c)=\frac{\operatorname{count}(w_i,c)}{\sum_{w\in V}\operatorname{count}(w,c)}\tag{4.12}
$$
其中词表 \(V\) 是**所有类别的词型并集**，而不是仅类别 \(c\) 中出现的词。

### 零概率问题

设想要估计在**正向**类别下词 “fantastic” 的概率，但训练集中**没有**任何“既包含 *fantastic* 且被标为正向”的文档（也许 *fantastic* 在负向里以讽刺方式出现）。这时该特征的极大似然概率为 0：
$$
\widehat{P}(\text{``fantastic''}\mid \text{positive})
=\frac{\operatorname{count}(\text{``fantastic''},\text{positive})}{\sum_{w\in V}\operatorname{count}(w,\text{positive})}
=0\tag{4.13}
$$

而朴素贝叶斯会把**所有特征的似然相乘**，因此只要某一特征在某一类别下概率为 0，那个类别的整体概率就被置 0，不管其它证据如何。

### 拉普拉斯（加一）平滑

最简单的解决方案是**加一平滑（Laplace smoothing）**。虽然在语言建模中常用更复杂的平滑，但在朴素贝叶斯文本分类中，加一平滑十分常见：
$$
\widehat{P}(w_i\mid c)
=\frac{\operatorname{count}(w_i,c)+1}{\sum_{w\in V}\big(\operatorname{count}(w,c)+1\big)}
=\frac{\operatorname{count}(w_i,c)+1}{\sum_{w\in V}\operatorname{count}(w,c)+|V|}\tag{4.14}
$$

> count (wi , c) 把训练集中所有属于c类别的文档拼成一个大文档后 词wi出现的次数 - > 防止一些0的数值出现 导致最后使用native 贝叶斯的时候出现0的情况

### 未登录词（unknown word）与停用词（stop words）

- **测试集出现但训练词表没有**的词（未登录词）：做法是**忽略**它们——从测试文档中删除，对概率计算不作贡献。  
- **停用词**（如 *the, a* 等高频词）：有些系统会把训练集中**最频繁的前 10–100 个词**或网上的停用词表作为停用词，并在训练与测试阶段都直接**丢弃**。但在多数文本分类任务中，使用停用词表**并不会带来性能提升**，更常见的做法是**使用完整词表**而不额外去除停用词。

# 朴素贝叶斯：加一平滑（add‑one）训练与测试示例

我们在**情感分析**场景中演示朴素贝叶斯（两类：正向 `+` 与负向 `−`），并使用**加一平滑**。

## 数据

**训练集**（简化自影评）：

- `−` : *just plain boring*  
- `−` : *entirely predictable and lacks energy*  
- `−` : *no surprises and very few laughs*  
- `+` : *very powerful*  
- `+` : *the most fun film of the summer*  

**测试句子**：  
`?` : *predictable with no fun*

其中单词 **with** 在训练集中**未出现**（未登录词），故在朴素贝叶斯中**直接丢弃**，测试句子视为：  
*S = “predictable no fun”*。

---

## 先验概率 \(P(c)\)

由式 (4.11)，用文档频率估计：
- 训练集中正向文档数 \(N_{+}=2\)，负向 \(N_{-}=3\)，总文档数 \(N_{\text{doc}}=5\)  
$$
\widehat{P}(+)=\frac{2}{5},\qquad \widehat{P}(-)=\frac{3}{5}.
$$

---

## 条件概率 P(w | c)（加一平滑，式 4.14）

把同一类别下的训练文档**拼接**为一个“大文档”。记：  
- ==负向大文档的总词数 \(N_{-}=14\)，正向大文档的总词数 \(N_{+}=9\)==  
- ==词表大小 \(|V|=20\)（所有类别词型的**并集**）==

对测试中涉及的三词 *predictable*、*no*、*fun*，按式 (4.14)：
$$
\widehat{P}(w_i\mid c)=\frac{\operatorname{count}(w_i,c)+1}{N_c+|V|}.
$$

**负向（−）**：
$$
\begin{aligned}
\widehat{P}(\text{predictable}\mid -)&=\frac{1+1}{14+20},\\
\widehat{P}(\text{no}\mid -)&=\frac{1+1}{14+20},\\
\widehat{P}(\text{fun}\mid -)&=\frac{0+1}{14+20}.
\end{aligned}
$$

**正向（+）**：
$$
\begin{aligned}
\widehat{P}(\text{predictable}\mid +)&=\frac{0+1}{9+20},\\
\widehat{P}(\text{no}\mid +)&=\frac{0+1}{9+20},\\
\widehat{P}(\text{fun}\mid +)&=\frac{1+1}{9+20}.
\end{aligned}
$$

---

## 分类（式 4.9）

对测试句 \(S=\) “predictable no fun”，其似然为三词概率的乘积：
$$
P(S\mid c)=P(\text{predictable}\mid c)\,P(\text{no}\mid c)\,P(\text{fun}\mid c).
$$

带入先验并比较两类：
$$
\begin{aligned}
\widehat{P}(-)\,P(S\mid -)
&=\frac{3}{5}\cdot\frac{2}{14+20}\cdot\frac{2}{14+20}\cdot\frac{1}{14+20}
=\frac{3}{5}\cdot\frac{4}{34^3}\approx 6.1\times 10^{-5},\\[6pt]
\widehat{P}(+)\,P(S\mid +)
&=\frac{2}{5}\cdot\frac{1}{9+20}\cdot\frac{1}{9+20}\cdot\frac{2}{9+20}
=\frac{2}{5}\cdot\frac{2}{29^3}\approx 3.2\times 10^{-5}.
\end{aligned}
$$

因此 \(\widehat{P}(-)\,P(S\mid -)>\widehat{P}(+)\,P(S\mid +)\)，模型判为 **负向（negative）**。

> 备注：文本分类中常见做法是在**对数域**累加对数概率以避免下溢；未登录词删除；是否使用停用词表对性能影响因任务而异。